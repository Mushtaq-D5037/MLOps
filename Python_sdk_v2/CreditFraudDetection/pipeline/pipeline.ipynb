{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the pipeline resources\r\n",
        "\r\n",
        "The Azure Machine Learning framework can be used from CLI, Python SDK, or studio interface. In this example, you use the Azure Machine Learning Python SDK v2 to create a pipeline. \r\n",
        "\r\n",
        "Before creating the pipeline, you need the following resources:\r\n",
        "\r\n",
        "* The data asset for training\r\n",
        "* The software environment to run the pipeline\r\n",
        "* A compute resource to where the job runs\r\n",
        "\r\n",
        "## Create handle to workspace\r\n",
        "\r\n",
        "Before we dive in the code, you need a way to reference your workspace. You'll create `ml_client` for a handle to the workspace.  You'll then use `ml_client` to manage resources and jobs.\r\n",
        "\r\n",
        "In the next cell, enter your Subscription ID, Resource Group name and Workspace name. To find these values:\r\n",
        "\r\n",
        "1. In the upper right Azure Machine Learning studio toolbar, select your workspace name.\r\n",
        "1. Copy the value for workspace, resource group and subscription ID into the code.\r\n",
        "1. You'll need to copy one value, close the area and paste, then come back for the next one.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\r\n",
        "from azure.identity import DefaultAzureCredential\r\n",
        "\r\n",
        "# authenticate\r\n",
        "credential = DefaultAzureCredential()\r\n",
        "# # Get a handle to the workspace\r\n",
        "ml_client = MLClient( credential=credential,\r\n",
        "                      subscription_id=\"<subcription_id>\",\r\n",
        "                      resource_group_name=\"<resource_group_name>\",\r\n",
        "                      workspace_name=\"<workspace_name>\",\r\n",
        "                    )\r\n",
        "\r\n",
        "# FETCH DATA\r\n",
        "credit_data = ml_client.data.get(\"credit_fraud_detection\", version='1')"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1688978221285
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a compute resource to run your pipeline\r\n",
        "\r\n",
        "You can **skip this step** if you want to use **serverless compute (preview)** to run the training job. Through serverless compute, Azure Machine Learning takes care of creating, scaling, deleting, patching and managing compute, along with providing managed network isolation, reducing the burden on you. \r\n",
        "\r\n",
        "Each step of an Azure Machine Learning pipeline can use a different compute resource for running the specific job of that step. It can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\r\n",
        "\r\n",
        "In this section, you provision a Linux  [compute cluster](https://docs.microsoft.com/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python). See the [full list on VM sizes and prices](https://azure.microsoft.com/en-ca/pricing/details/machine-learning/) .\r\n",
        "\r\n",
        "For this tutorial, you only need a basic cluster so use a Standard_DS3_v2 model with 2 vCPU cores, 7-GB RAM and create an Azure Machine Learning Compute.\r\n",
        "> [!TIP]\r\n",
        "> If you already have a compute cluster, replace \"cpu-cluster\" in the next code block with the name of your cluster.  This will keep you from creating another one.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\r\n",
        "\r\n",
        "# Name assigned to the compute cluster\r\n",
        "cpu_compute_target = \"ML-Pipeline-Cluster\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # let's see if the compute target already exists\r\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\r\n",
        "    print(f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\")\r\n",
        "\r\n",
        "except Exception:\r\n",
        "    print(\"Creating a new cpu compute target...\")\r\n",
        "\r\n",
        "    # Let's create the Azure Machine Learning compute object with the intended parameters\r\n",
        "    # if you run into an out of quota error, change the size to a comparable VM that is available.\r\n",
        "    # Learn more on https://azure.microsoft.com/en-us/pricing/details/machine-learning/.\r\n",
        "    cpu_cluster = AmlCompute(\r\n",
        "        name=cpu_compute_target,\r\n",
        "        # Azure Machine Learning Compute is the on-demand VM service\r\n",
        "        type=\"amlcompute\",\r\n",
        "        # VM Family\r\n",
        "        size=\"STANDARD_DS3_V2\",\r\n",
        "        # Minimum running nodes when there is no job running\r\n",
        "        min_instances=0,\r\n",
        "        # Nodes in cluster\r\n",
        "        max_instances=4,\r\n",
        "        # How many seconds will the node running after the job termination\r\n",
        "        idle_time_before_scale_down=180,\r\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\r\n",
        "        tier=\"Dedicated\",\r\n",
        "    )\r\n",
        "    print(f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\")\r\n",
        "    # Now, we pass the object to MLClient's create_or_update method\r\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named ML-Pipeline-Cluster, we'll reuse it as is.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688978224661
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a job environment for pipeline steps\r\n",
        "\r\n",
        "So far, you've created a development environment on the compute instance, your development machine. You also need an environment to use for each step of the pipeline. Each step can have its own environment, or you can use some common environments for multiple steps.\r\n",
        "\r\n",
        "In this example, you create a conda environment for your jobs, using a conda yaml file.\r\n",
        "First, create a directory to store the file in."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\r\n",
        "\r\n",
        "custom_env_name  = \"aml-scikit-learn\"\r\n",
        "ver = \"0.1.1\"\r\n",
        "# dependencies_dir = './dependencies'\r\n",
        "# pipeline_job_env = Environment( name=custom_env_name,\r\n",
        "#                                 description=\"Custom environment for Credit Card Defaults pipeline\",\r\n",
        "#                                 tags={\"scikit-learn\": \"0.24.2\"},\r\n",
        "#                                 conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\r\n",
        "#                                 image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\r\n",
        "#                                 version=ver,\r\n",
        "#                               )\r\n",
        "# pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\r\n",
        "\r\n",
        "# GET ENVIRONMENT\r\n",
        "pipeline_job_env = ml_client.environments.get(name=custom_env_name, version=ver)\r\n",
        "\r\n",
        "\r\n",
        "print(f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name aml-scikit-learn is registered to workspace, the environment version is 0.1.1\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688978229518
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the training pipeline\r\n",
        "\r\n",
        "Now that you have all assets required to run your pipeline, it's time to build the pipeline itself.\r\n",
        "\r\n",
        "Azure Machine Learning pipelines are reusable ML workflows that usually consist of several components. The typical life of a component is:\r\n",
        "\r\n",
        "- Write the yaml specification of the component, or create it programmatically using `ComponentMethod`.\r\n",
        "- Optionally, register the component with a name and version in your workspace, to make it reusable and shareable.\r\n",
        "- Load that component from the pipeline code.\r\n",
        "- Implement the pipeline using the component's inputs, outputs and parameters.\r\n",
        "- Submit the pipeline.\r\n",
        "\r\n",
        "There are two ways to create a component, programmatic and yaml definition. The next two sections walk you through creating a component using programmatic definition\r\n",
        "\r\n",
        "> [!NOTE]\r\n",
        "> In this tutorial for simplicity we are using the same compute for all components. However, you can set different computes for each component, for example by adding a line like `train_step.compute = \"cpu-cluster\"`. To view an example of building a pipeline with different computes for each component, see the [Basic pipeline job section in the cifar-10 pipeline tutorial](https://github.com/Azure/azureml-examples/blob/main/sdk/python/jobs/pipelines/2b_train_cifar_10_with_pytorch/train_cifar_10_with_pytorch.ipynb).\r\n",
        "\r\n",
        "### Create component: data preparation (using programmatic definition)\r\n",
        "\r\n",
        "Let's start by creating the first component. This component handles the preprocessing of the data. The preprocessing task is performed in the *data_preparation.py* Python file.\r\n",
        "\r\n",
        "First create a source folder for the data_prep component:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\r\n",
        "from azure.ai.ml import Input, Output\r\n",
        "\r\n",
        "scripts_dir = \"./src\"\r\n",
        "data_prep_component = command( name=\"Data prep CreditFraud Detection\",\r\n",
        "                               display_name =\"Data preparation for training\",\r\n",
        "                               description  =\"reads input data & preprocesses it\",\r\n",
        "                               inputs= { \"data\": Input(type=\"uri_folder\") },\r\n",
        "                               outputs=dict( processed_data=Output(type=\"uri_folder\", mode=\"rw_mount\")),\r\n",
        "                               # The source folder of the component\r\n",
        "                               code=scripts_dir,\r\n",
        "                               command=\"\"\"python data_preparation.py \\\r\n",
        "                                        --data ${{inputs.data}} \\\r\n",
        "                                        --processed_data ${{outputs.processed_data}} \\\r\n",
        "                                        \"\"\",\r\n",
        "                               environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\r\n",
        "                            )\r\n",
        "\r\n",
        "train_component = command( name=\"Training  Model\",\r\n",
        "                            display_name =\"Training Model\",\r\n",
        "                            # description  =\"reads input data & preprocesses it\",\r\n",
        "                            inputs= { \"processed_data\": Input(type=\"uri_folder\"),\r\n",
        "                                      \"test_train_ratio\": Input(type='number'),\r\n",
        "                                      \"registered_model_name\":Input(type='string'),\r\n",
        "                                    },\r\n",
        "                            outputs=dict(model=Output(type=\"uri_folder\", mode=\"rw_mount\")),\r\n",
        "                            # The source folder of the component\r\n",
        "                            code=scripts_dir,\r\n",
        "                            command=\"\"\"python train.py \\\r\n",
        "                                    --input_data ${{inputs.processed_data}} \\\r\n",
        "                                    --registered_model_name ${{inputs.registered_model_name}} \\\r\n",
        "                                    --model ${{outputs.model}} \\\r\n",
        "                                    \"\"\",\r\n",
        "                            environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\r\n",
        "                            )"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688981513953
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Pipeline using Components\r\n",
        "To code the pipeline, you use a specific `@dsl.pipeline` decorator that identifies the Azure Machine Learning pipelines. In the decorator, we can specify the pipeline description and default resources like compute and storage. Like a Python function, pipelines can have inputs. You can then create multiple instances of a single pipeline with different inputs.\r\n",
        "\r\n",
        "Here, we used *input data*, *split ratio* and *registered model name* as input variables. We then call the components and connect them via their inputs/outputs identifiers. The outputs of each step can be accessed via the `.outputs` property.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\r\n",
        "from azure.ai.ml import dsl, Input, Output\r\n",
        "\r\n",
        "\r\n",
        "@dsl.pipeline(compute=cpu_compute_target, description=\"E2E data_prep-train pipeline\")\r\n",
        "def credit_fraud_detection_pipeline(input_data, test_train_ratio, registered_model_name,):\r\n",
        "                             # using data_prep_function like a python call with its own inputs\r\n",
        "                             data_prep_job = data_prep_component(data=input_data,)\r\n",
        "\r\n",
        "                             # using train_func like a python call with its own inputs\r\n",
        "                             train_job = train_component( processed_data  = data_prep_job.outputs.processed_data,     # note: using outputs from previous step\r\n",
        "                                                          test_train_ratio=test_train_ratio,\r\n",
        "                                                          registered_model_name=registered_model_name,\r\n",
        "                                                        )\r\n",
        "\r\n",
        "                             # a pipeline returns a dictionary of outputs\r\n",
        "                             # keys will code for the pipeline output identifier\r\n",
        "                             # return  { \"processed_data\": data_prep_job.outputs.processed_data }"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688981522316
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initiate Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model_name = \"FraudDetectionModel\"\r\n",
        "\r\n",
        "# Let's instantiate the pipeline with the parameters of our choice\r\n",
        "pipeline = credit_fraud_detection_pipeline(input_data=Input(type=\"uri_file\", path=credit_data.path),\r\n",
        "                                    test_train_ratio=0.25,\r\n",
        "                                    registered_model_name=registered_model_name,\r\n",
        "                                    )"
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688981525505
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit the job \r\n",
        "\r\n",
        "It's now time to submit the job to run in Azure Machine Learning. This time you use `create_or_update`  on `ml_client.jobs`.\r\n",
        "\r\n",
        "Here you also pass an experiment name. An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure Machine Learning studio.\r\n",
        "\r\n",
        "Once completed, the pipeline registers a model in your workspace as a result of training."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submit the pipeline job\r\n",
        "pipeline_job = ml_client.jobs.create_or_update(pipeline,experiment_name=\"e2e_registered_components\",)\r\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: keen_arm_rd3wjr89rm\nWeb View: https://ml.azure.com/runs/keen_arm_rd3wjr89rm?wsid=/subscriptions/4c3b2838-71d0-44f4-9f40-4539213bfcf4/resourcegroups/rg-dev-allerganconnect/workspaces/Clustering_Analysis\n\nStreaming logs/azureml/executionlogs.txt\n========================================\n\n[2023-07-10 09:32:14Z] Completing processing run id d56fefbb-fe18-4e29-9141-33fa4db6b163.\n[2023-07-10 09:32:15Z] Submitting 1 runs, first five are: 1538f8dd:7edb7d7d-85c9-469d-bfe5-2f33c6a104c9\n[2023-07-10 09:40:53Z] Completing processing run id 7edb7d7d-85c9-469d-bfe5-2f33c6a104c9.\n\nExecution Summary\n=================\nRunId: keen_arm_rd3wjr89rm\nWeb View: https://ml.azure.com/runs/keen_arm_rd3wjr89rm?wsid=/subscriptions/4c3b2838-71d0-44f4-9f40-4539213bfcf4/resourcegroups/rg-dev-allerganconnect/workspaces/Clustering_Analysis\n\n"
        }
      ],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688982104102
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "print(os.getcwd())\r\n",
        "# dir  = os.getcwd()\r\n",
        "# dir1 = os.chdir('../dir')\r\n",
        "print('Dir:',dir)\r\n",
        "# print('Dir1:',dir1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/computeinstance-m/code/Users/musthaq.mohammed/AzureML/CreditFraudDetection\nDir: /mnt/batch/tasks/shared/LS_root/mounts/clusters/computeinstance-m/code/Users/musthaq.mohammed/AzureML/CreditFraudDetection\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688978929817
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
